# -*- coding: utf-8 -*-

import unicodedata


class UnicodeTokenizer:
    def __init__(self,  do_lower_case=False, never_split=[], highUnicodePoint=8192, strip=False, add_dummy_prefix=False):
        self.do_lower_case = do_lower_case
        self.highUnicodePoint = highUnicodePoint
        self.strip = strip
        self.add_dummy_prefix = add_dummy_prefix
        self.never_split = set(x for x in never_split)

    def split_blank(self, line):
        t = []
        for i, c in enumerate(line):
            if unicodedata.category(c)[0] == 'Z':
                if i==0 or line[i-1]!=c:
                    t.append(c)
                else:
                    t[-1] += c
                if i < len(line)-1 and line[i+1] != c:
                    t.append('')
            elif ord(c) >= self.highUnicodePoint:
                t += ['', c, '']
            else:
                if not t:
                    t.append(c)
                else:
                    t[-1] += c
        if self.strip:
            tokens = [x.strip() for x in t if x.strip()]
        else:
            tokens = [x for x in t if x]
        return tokens

    def split_category(self, line):
        if len(line) == 1:
            return [line]
        categorys = [unicodedata.category(x) for x in line]
        names = [unicodedata.name(x).split()[0] if categorys[i][0] in 'LN' else None for i, x in enumerate(line)]
        tokens = []
        for i, x in enumerate(line):
            if i == 0:
                tokens.append(x)
            elif categorys[i][0] == 'Z':
                tokens.append(x)
            elif categorys[i] == 'Mn':
                tokens[-1] +=x
            elif categorys[i][0] == categorys[i-1][0] and names[i] == names[i-1]:
                tokens[-1] += x
            else:
                tokens.append(x)

        tokens = [x for x in tokens if x]
        return tokens

    def tokenize(self, line):
        if len(line) <= 1:
            return list(line)
        if self.add_dummy_prefix and line[0] != ' ' and ord(line[0]) < self.highUnicodePoint and unicodedata.category(line[1])[0] == 'L':
            line = ' '+line
        words = self.split_blank(line)
        tokens = []
        for i, w in enumerate(words):
            if w in self.never_split:
                if tokens and tokens[-1][-1]==' ':
                    tokens[-1]=tokens[-1][:-1]
                tokens.append(w)
            else:
                u = w
                if self.do_lower_case:
                    u = unicodedata.normalize("NFD", u.lower())
                t = self.split_category(u)
                c = t[0][0]
                if tokens and tokens[-1] == ' ' and ord(c) < self.highUnicodePoint and unicodedata.category(c)[0] == 'L':
                    tokens[-1] += t[0]
                    tokens += t[1:]
                else:
                    tokens += t
        return tokens

    def detokenize(self, tokens):
        l = ''
        for i, x in enumerate(tokens):
            if i == 0 and self.add_dummy_prefix and len(x) >= 2 and x.startswith(' ') and ord(x[1]) < self.highUnicodePoint and unicodedata.category(x[1])[0] == 'L':
                x = x[1:]
            if x in self.never_split:
                l += ' '+x
            else:
                l += x
        return l


def test_UnicodeTokenizer():
    doc = ["ï¡¿'ã€‡ãŽ¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ðŸ˜€ç†‡'\x0000ð§­2022ï¼’ï¼ï¼‘ï¼™\U0010ffff",
           "á„ƒá…¢á„’á…¡á†«á„†á…µá†«á„€á…®á†¨á„‹á…´â…§é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name",
           "çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis",
           "í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìžˆì–´ìš”ê†ƒêŽ­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† [MASK]   [PAD] [CLS][SEP]",
           '''est ð—´‚ð—¹­ð˜œ¶ð—´²ð—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹).''',
           'à¸§à¸£à¸£à¸“à¸žà¸‡à¸©à¹Œà¹€à¸›à¹‡à¸™à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸Šà¸±à¹‰à¸™à¸›à¸µà¸—à¸µà¹ˆà¸«à¸™à¸¶à¹ˆà¸‡ à¹€à¸£à¸µà¸¢à¸™à¸ªà¸²à¸‚à¸²à¸§à¸´à¸—à¸¢à¸²à¸à¸²à¸£à¸„à¸­à¸¡à¸žà¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¹à¸¥à¸°à¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨à¸„à¸“à¸°à¸§à¸´à¸—à¸¢à¸²à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹à¸¥à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¸‚à¸­à¸™à¹à¸à¹ˆà¸™à¸§à¸´à¸—à¸¢à¸²à¹€à¸‚à¸•à¸«à¸™à¸­à¸‡à¸„à¸²à¸¢à¸¢à¸·à¸¡à¸„à¸·à¸™à¸—à¸£à¸±à¸žà¸¢à¸²à¸à¸£à¸«à¹‰à¸­à¸‡à¸ªà¸¡à¸¸à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸±à¸¡à¸¡à¸™à¸²à¸„à¸­à¸¡à¸žà¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¸à¸±à¸šà¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¹€à¸à¸¡à¹à¸¡à¸§à¸à¸´à¸™à¸›à¸¥à¸²à¸«à¸´à¸§à¸§à¸§à¹„à¸«à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£à¹ƒà¸«à¸¡à¹ˆà¸ªà¸”à¸ªà¸”à¸—à¸™à¹„à¸”à¹‰',
           'àºªàº»àº¡à»€àº”àº±àº”àºžàº°à»€àºˆàº»à»‰àº²àº¢àº¹à»ˆàº«àº»àº§àºšà»àº£àº»àº¡à»‚àºàº”àºŠàº»àº‡àº—àº³àº™àº¸àºšàº³àº¥àº¸àº‡àºšà»‰àº²àº™à»€àº¡àº·àº­àº‡à»àº¥àº°àºžàº°àºªàº²àº”àºªàº°à»œàº²àºˆàº»àº™àºà»ˆàº²àº§à»„àº”à»‰àº§à»ˆàº²àºàº¸àº‡àºªàºµàº­àº°àºàº¸àº—àº°àº¢àº²à»ƒàº™àºªàº°à»„à»àºžàº°àº­àº»àº‡àº™àº±à»‰àº™à»€àº›àº±àº™àºàº¸àºàº—àºµà»ˆàºšà»‰àº²àº™à»€àº¡àº·àº­àº‡àº”àºµ àº¡àºµàº‚àº¸àº™àº™àº²àº‡àº„àº»àº™àºªàº³àº„àº±àº™àº—àºµà»ˆà»€àº•àºµàºšà»‚àº•à»ƒàº™à»€àº§àº¥àº²àº•à»à»ˆàº¡àº² à»ƒàº™àº¥àº²àºŠàº°àºàº²àº™àº‚àº­àº‡àºžàº°àº­àº»àº‡àº«àº¼àº²àºàº„àº»àº™ à»€àºŠàº±à»ˆàº™ àºªàº»àº¡à»€àº”àº±àº”àºžàº°à»€àºˆàº»à»‰àº²àºàº¸àº‡àº—àº»àº™àºšàº¸àº¥àºµ, àºžàº°àºšàº²àº”àºªàº»àº¡à»€àº”àº±àº”àºžàº°àºžàº¸àº”àº—àº°àºàº­àº”àºŸà»‰àº²àºˆàº¸àº¥àº²à»‚àº¥àºàº¡àº°àº«àº²àº¥àº²àº” à»€àº›àº±àº™àº•àº»à»‰àº™ à»ƒàº™àº—àº²àº‡àº”à»‰àº²àº™àº§àº±àº™àº™àº°àº„àº°àº”àºµàºà»àº¡àºµàºàº°àº§àºµàº„àº»àº™àºªàº³àº„àº±àº™ à»€àºŠàº±à»ˆàº™ à»€àºˆàº»à»‰àº²àºŸà»‰àº²àº—àº³àº¡àº²àº—àº´à»€àºšàº”à»„àºŠàºàº°à»€àºŠàº”àºªàº¸àº¥àº´àºàº°àº§àº»àº‡ àºàº»àº¡àº¡àº°àº‚àº¸àº™à»€àºªàº™àº²àºžàº´àº—àº±àº àº«àº¼àº·à»€àºˆàº»à»‰àº²àºŸà»‰àº²àºàº¸à»‰àº‡ à»€àºŠàº´à»ˆàº‡à»€àº›àº±àº™àºžàº°à»‚àº­àº¥àº»àº” à»€àº›àº±àº™àº•àº»à»‰àº™'
           ]
    # unicodeTokenizer = UnicodeTokenizer()
    unicodeTokenizer = UnicodeTokenizer(never_split=["[UNK]", "[SEP]", "[PAD]", "[CLS]", "[MASK]"])
    for line in doc:
        tokens = unicodeTokenizer.tokenize(line)
        logger.info(line)
        logger.info(unicodeTokenizer.detokenize(tokens))
        logger.info((tokens))


if __name__ == "__main__":
    from logzero import logger

    # line = "art_new_word=True"
    line = "Hello word"
    tokenizer = UnicodeTokenizer()
    logger.info((tokenizer.split_blank(line)))

    logger.info(tokenizer.tokenize("word"))
    tokens = tokenizer.tokenize(line)
    logger.info(tokens)
    logger.info(tokenizer.detokenize(tokens))

    line = "á„ƒá…¢á„’á…¡á†«á„†á…µá†«á„€á…®á†¨á„‹á…´ï¡¿'ã€‡ãŽ¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ðŸ˜€ç†‡'\x0000ð§­2022ï¼’ï¼ï¼‘ï¼™\U0010ffff"
    tokenizer = UnicodeTokenizer(strip=True)
    tokens = tokenizer.tokenize(line)
    logger.info(tokens)
    logger.info(tokenizer.detokenize(tokens))
    import timeit
    # re=timeit.timeit("''.join(chr(x) for x in range(int(1e6))) ")
    # logger.info(re)
    tokenizer = UnicodeTokenizer()

    # import time
    # t0 = time.time()
    # for i in range(10000):
    #     # chr(i)  # ValueError: chr() arg not in range(0x110000)
    #     tokenizer.tokenize(line)
    # t1 = time.time()
    # logger.info(t1-t0)

    test_UnicodeTokenizer()

"""
 ï¡¿'ã€‡ãŽ¡[à¸„ à¸“ à¸ˆà¸°à¸ˆ à¸” à¸ž à¸˜ à¹ à¸• à¸‡ à¸‡à¸²à¸™à¹€à¸¡ à¸­ à¹„à¸£à¸„à¸°]â…·pays-g[ran]d-blanc-e l eveÂ»(ç™½é«˜å¤§å¤åœ‹)ðŸ˜€ç†‡'00ð§­ï¼’ï¼ï¼‘ï¼™ô¿¿
 â–ã€€â–
"""
