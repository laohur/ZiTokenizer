import collections
import os
import gc
import multiprocessing

from logzero import logger

from ZiTokenizer.UnicodeTokenizer import UnicodeTokenizer
from ZiTokenizer.ZiSegmenter import ZiSegmenter
from ZiTokenizer.ZiCutter import ZiCutter
from ZiTokenizer.ZiTokenizer import ZiTokenizer
from glance import load_frequency, describe

doc = ["ï¡¿'ã€‡ãŽ¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ðŸ˜€ç†‡'\x0000ð§­2022ï¼’ï¼ï¼‘ï¼™\U0010ffff",
       "á„ƒá…¢á„’á…¡á†«á„†á…µá†«á„€á…®á†¨á„‹á…´â…§é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name",
       "çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis",
       "í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìžˆì–´ìš”ê†ƒêŽ­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† [MASK] [PAD] [CLS][SEP]",
       '''est ð—´‚ð—¹­ð˜œ¶ð—´²ð—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹).''',
       'à¸§à¸£à¸£à¸“à¸žà¸‡à¸©à¹Œà¹€à¸›à¹‡à¸™à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸Šà¸±à¹‰à¸™à¸›à¸µà¸—à¸µà¹ˆà¸«à¸™à¸¶à¹ˆà¸‡ à¹€à¸£à¸µà¸¢à¸™à¸ªà¸²à¸‚à¸²à¸§à¸´à¸—à¸¢à¸²à¸à¸²à¸£à¸„à¸­à¸¡à¸žà¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¹à¸¥à¸°à¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨à¸„à¸“à¸°à¸§à¸´à¸—à¸¢à¸²à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹à¸¥à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¸‚à¸­à¸™à¹à¸à¹ˆà¸™à¸§à¸´à¸—à¸¢à¸²à¹€à¸‚à¸•à¸«à¸™à¸­à¸‡à¸„à¸²à¸¢à¸¢à¸·à¸¡à¸„à¸·à¸™à¸—à¸£à¸±à¸žà¸¢à¸²à¸à¸£à¸«à¹‰à¸­à¸‡à¸ªà¸¡à¸¸à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸±à¸¡à¸¡à¸™à¸²à¸„à¸­à¸¡à¸žà¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¸à¸±à¸šà¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¹€à¸à¸¡à¹à¸¡à¸§à¸à¸´à¸™à¸›à¸¥à¸²à¸«à¸´à¸§à¸§à¸§à¹„à¸«à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£à¹ƒà¸«à¸¡à¹ˆà¸ªà¸”à¸ªà¸”à¸—à¸™à¹„à¸”à¹‰',
       'àºªàº»àº¡à»€àº”àº±àº”àºžàº°à»€àºˆàº»à»‰àº²àº¢àº¹à»ˆàº«àº»àº§àºšà»àº£àº»àº¡à»‚àºàº”àºŠàº»àº‡àº—àº³àº™àº¸àºšàº³àº¥àº¸àº‡àºšà»‰àº²àº™à»€àº¡àº·àº­àº‡à»àº¥àº°àºžàº°àºªàº²àº”àºªàº°à»œàº²àºˆàº»àº™àºà»ˆàº²àº§à»„àº”à»‰àº§à»ˆàº²àºàº¸àº‡àºªàºµàº­àº°àºàº¸àº—àº°àº¢àº²à»ƒàº™àºªàº°à»„à»àºžàº°àº­àº»àº‡àº™àº±à»‰àº™à»€àº›àº±àº™àºàº¸àºàº—àºµà»ˆàºšà»‰àº²àº™à»€àº¡àº·àº­àº‡àº”àºµ àº¡àºµàº‚àº¸àº™àº™àº²àº‡àº„àº»àº™àºªàº³àº„àº±àº™àº—àºµà»ˆà»€àº•àºµàºšà»‚àº•à»ƒàº™à»€àº§àº¥àº²àº•à»à»ˆàº¡àº² à»ƒàº™àº¥àº²àºŠàº°àºàº²àº™àº‚àº­àº‡àºžàº°àº­àº»àº‡àº«àº¼àº²àºàº„àº»àº™ à»€àºŠàº±à»ˆàº™ àºªàº»àº¡à»€àº”àº±àº”àºžàº°à»€àºˆàº»à»‰àº²àºàº¸àº‡àº—àº»àº™àºšàº¸àº¥àºµ, àºžàº°àºšàº²àº”àºªàº»àº¡à»€àº”àº±àº”àºžàº°àºžàº¸àº”àº—àº°àºàº­àº”àºŸà»‰àº²àºˆàº¸àº¥àº²à»‚àº¥àºàº¡àº°àº«àº²àº¥àº²àº” à»€àº›àº±àº™àº•àº»à»‰àº™ à»ƒàº™àº—àº²àº‡àº”à»‰àº²àº™àº§àº±àº™àº™àº°àº„àº°àº”àºµàºà»àº¡àºµàºàº°àº§àºµàº„àº»àº™àºªàº³àº„àº±àº™ à»€àºŠàº±à»ˆàº™ à»€àºˆàº»à»‰àº²àºŸà»‰àº²àº—àº³àº¡àº²àº—àº´à»€àºšàº”à»„àºŠàºàº°à»€àºŠàº”àºªàº¸àº¥àº´àºàº°àº§àº»àº‡ àºàº»àº¡àº¡àº°àº‚àº¸àº™à»€àºªàº™àº²àºžàº´àº—àº±àº àº«àº¼àº·à»€àºˆàº»à»‰àº²àºŸà»‰àº²àºàº¸à»‰àº‡ à»€àºŠàº´à»ˆàº‡à»€àº›àº±àº™àºžàº°à»‚àº­àº¥àº»àº” à»€àº›àº±àº™àº•àº»à»‰àº™'
       ]


def test_UnicodeTokenizer():
    tokenizer = UnicodeTokenizer(never_split=["[UNK]", "[SEP]", "[PAD]", "[CLS]", "[MASK]"])
    for l in doc:
        t = tokenizer.tokenize(l)
        logger.info((len(l), l))
        logger.info((len(t), ' '.join(t)))
    tokenizer = UnicodeTokenizer(split_digit=True)
    for l in doc:
        t = tokenizer.tokenize(l)
        logger.info((len(l), l))
        logger.info((len(t), ' '.join(t)))

def test_ZiCutter(dir):
    cutter = ZiCutter(dir=dir)
    line = "ï¡¿'ã€‡ãŽ¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ç†µðŸ˜€'\x0000ç†‡"
    tokens = cutter.tokenize(line)
    logger.info(tokens)
    a = cutter.combineHanzi(tokens)
    b = cutter.combineHanzi(''.join(tokens))
    logger.info(a)
    logger.info(b)


def test_build(freq_path, dir, alpha=1):
    if not os.path.exists(dir):
        os.makedirs(dir)
    import logzero
    logzero.logfile(os.path.join(dir, "ZiTokenizerBuild.log"), mode='w')
    tokenizer = ZiTokenizer()
    logger.warning(f" building from {freq_path}")
    word_freq, total = load_frequency(freq_path, alpha=alpha, do_lower_case=tokenizer.do_lower_case)
    assert word_freq
    P = []
    Q = []
    for x in word_freq.items():
        if x[1] >= 2:
            P.append(x)
        else:
            Q.append(x)
    del word_freq
    gc.collect()
    P.sort(key=lambda x: (-x[1], len(x[0]), x[0]))
    Q.sort(key=lambda x: (-x[1], len(x[0]), x[0]))
    word_freq = P+Q
    del P, Q
    gc.collect()
    describe(word_freq, total)
    tokenizer.build(word_freq, total, dir)

    tokenizer = ZiTokenizer(dir)
    doc = os.popen(f" tail {freq_path} -n 1000000 | shuf -n 10 ").read().splitlines()
    doc = [x.split('\t') for x in doc]
    doc = [(k, float(v)) for k, v in doc]
    for k, v in doc:
        tokens = tokenizer.tokenize(k)
        indexs = tokenizer.encode(k)
        words = tokenizer.decode(indexs)
        row = [k, v, ' '.join(tokens), tokenizer.token_word(k), ' '.join(words)]
        logger.info((row))
    return tokenizer


def test_line(tokenizer):
    for line in doc:
        logger.info(line)
        tokens = tokenizer.tokenize(line)
        logger.info(' '.join(tokens))

        indexs = tokenizer.encode(line)
        tokens = tokenizer.decode(indexs)
        line2=tokenizer.tokens2line(tokens)
        logger.info(' '.join(tokens))
        if line2 == line:
            logger.info(line2)
        else:
            logger.warning(line2)


def get_langs():
    langs = os.listdir('C:/data/languages')
    langs = [x for x in langs if not x.startswith('global')]
    langs1 = []
    for lang in langs:
        freq_path = f"C:/data/languages/{lang}/word_frequency.tsv"
        if not os.path.exists(freq_path):
            continue
        langs1.append((lang, freq_path))
    return langs1


if __name__ == "__main__":

    test_UnicodeTokenizer()

    tokenizer = ZiTokenizer()
    test_line(tokenizer)

    tokenizer = ZiTokenizer(split_digit=True)
    test_line(tokenizer)

    folder = "./demo"
    freq_path = f"{folder}/word_frequency.tsv"  # lang="zh_classical"
    test_ZiCutter(folder)
    test_build(freq_path, folder)

    tokenizer = ZiTokenizer(folder)
    test_line(tokenizer)


"""

"""
