# ZiTokenizer

ZiTokenizer: tokenize word as Zi

read word as Zi

word = prefix + root + suffix

support 175 languages + global 
## use
* pip install ZiTokenizer
* toeknize language frequency and count word frequency (https://github.com/laohur/UnicodeTokenizer/blob/master/test/count_lang/count_word.py)
```python
from ZiTokenizer.ZiTokenizer import ZiTokenizer

# use
tokenizer = ZiTokenizer(lang="global")  # lang='ar', 'en', 'fr', 'ru', 'zh' ...
line = "ï¡¿'ã€‡ã¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ç†µğŸ˜€'\x0000ç†‡"
tokens = tokenizer.tokenize(line)
print(' '.join(tokens)) # ' ã€‡ ã¡ [ à¸„ à¸“-- à¸ˆ-- à¸° --à¸ˆ à¸”-- à¸ à¸˜ à¹à¸• à¸‡-- à¸‡à¸²-- à¸™-- à¹€à¸¡ à¸­à¹„-- à¸£ --à¸„ --à¸° ] ##s ht pays - g [ ran ] d - blanc - eleve Â» ( ç™½ é«˜ å¤§ å¤ åœ‹ ) â¿° ç« å•† ##g ce ' 00 â¿° ç« é«˜

# build 
tokenizer = ZiTokenizer(mydir) # mydir include "word_frequency.tsv"
tokenizer.build(min_ratio=2e-6, min_freq=1)
tokenizer = ZiTokenizer(dir=mydir)

```
