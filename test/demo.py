import os

from logzero import logger

from UnicodeTokenizer.UnicodeTokenizer import UnicodeTokenizer
from ZiTokenizer.ZiSegmenter import ZiSegmenter
from ZiTokenizer.ZiTokenizer import ZiTokenizer


def get_langs():
    alphabet = ''.join(chr(x) for x in range(ord('a'), ord('z')+1))
    langs = [x+y for x in alphabet for y in alphabet]
    return langs


def test_segmenter():
    line = "ï¡¿'ã€‡ã¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ğŸ˜€ç†‡'\x0000ğ§­"
    roots = ['la', 'a', 'ay', 'le']
    prefixs = ['e', 'l']
    suffixs = ['n', 'e', 'v']
    words = UnicodeTokenizer().tokenize(line)
    cutter = ZiSegmenter(roots, prefixs, suffixs)
    for word in words:
        tokens = cutter.token_word(word)
        print(word, tokens)


def test_build(dir):
    freq_path = f"{dir}/word_frequency.tsv"
    if not os.path.exists(freq_path):
        logger.warning("no "+freq_path)
        return

    import logzero
    logzero.logfile(os.path.join(dir, "ZiTokenizerBuild.log"), mode='w')

    tokenizer = ZiTokenizer(dir)
    tokenizer.build(min_ratio=2e-6, min_freq=0)

    tokenizer = ZiTokenizer(dir)
    line = "ï¡¿'ã€‡ã¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ğŸ˜€ç†‡'\x0000ğ§­"
    tokens = tokenizer.tokenize(line)
    logger.info(' '.join(tokens))

    indexs = tokenizer.encode(line)
    words = tokenizer.decode(indexs)
    logger.info(' '.join(words))

    doc = os.popen(f" shuf {freq_path} -n 10").read().splitlines()
    doc = [x.split('\t') for x in doc]
    doc = [(k, int(v)) for k, v in doc]
    for k, v in doc:
        row = [k, v, tokenizer.cut(k)]
        logger.info((row))


def test_lang(lang):

    tokenizer = ZiTokenizer(lang=lang)

    line = "ï¡¿'ã€‡ã¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)ğŸ˜€ç†‡'\x0000ğ§­"
    tokens = tokenizer.tokenize(line)
    logger.info(' '.join(tokens))

    indexs = tokenizer.encode(line)
    words = tokenizer.decode(indexs)
    logger.info(' '.join(words))


if __name__ == "__main__":
    test_segmenter()
    langs0 = ['ho', 'ff', 'aa', 'kj', 'kl', 'mh', 'xh', 'zh', 'ja',
              'th', 'ar', 'en', 'fr', 'ru',   'global'][:]
    langs = get_langs()
    langs = [x for x in langs if x not in langs0]
    langs = langs0

    for lang in langs[:3]:
        test_build(dir=f"C:/data/languages/{lang}")
        # test_lang(lang)

"""
[I 220705 23:16:13 demo:30] ' ã€‡ #sq ed [ à¸„ à¸“- à¸ˆà¸°- à¸ˆ à¸”- à¸ #th ng à¹- à¸• à¸‡- à¸‡- à¸² -à¸™ -à¹€à¸¡ à¸­à¹„- à¸£ -à¸„ -à¸° ] #sm ht pays - g [ ran ] d - blanc - eleve Â» ( ç™½ é«˜ å¤§ å¤ åœ‹ ) #gr ce 
â¿° ç« é«˜ ' 00 â¿° è¨€ è‡º
[I 220705 23:16:13 demo:34] ' ã€‡ #sq ed [ à¸„ à¸“à¸ˆà¸°à¸ˆ à¸”à¸ #th ng à¹à¸• à¸‡à¸‡à¸²à¸™à¹€à¸¡ à¸­à¹„à¸£à¸„à¸° ] #sm ht pays - g [ ran ] d - blanc - eleve Â» ( ç™½ é«˜ å¤§ å¤ åœ‹ ) #gr ce â¿° ç« é«˜ ' 00 â¿° è¨€ è‡º

"""
